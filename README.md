# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
The dataset that is being trained on represents employment, demographic, and educational data concerning potential customers of a bank. The task of classification is to predict whether or not the potential customer will be likely to subscribe to term deposits with that bank.

The best performing model was a logisitic regression with parameters of C = {} and maximum iterations = {}. Logisitic regression provides a lightweight model capable of powerful binary classification performance. These parameters represented an optimal combination of the number of iterations allowed for solver convergence and an inverse regularization hyperparameter.

## Scikit-learn Pipeline
The pipeline for model training includes cleaning and One Hot Encoding the data, splitting into test and train sets, conducting random parameter sampling, and using a bandit early termination policy. The model performance is scored via accuracy. The hyperparameter tuner finds optimal values for the C parameter (related to inverse regularization and overfitting) as well as max_iter, or the maximum number of iterations allowed for solver convergence in the logistic model.

Random parameter sampling provides a more resource-conservative way to optimize parameter values. Rather than exhaustively test all possible value combinations, such as a grid search, random search can sample (in this case uniformly) across the entire range of parameter value combinations, allowing the parameter search to cover more or less the same parameter space as grid search at a lower cost. While bayesian search methods can use the score of values to update projected distributions, they can still prove to be more costly and less effective than random sampling.

The bandit policy utilized prevents the model training from wasting resources once model performance dips below a certain threshold. This threshold is related to the current best performing score. In this case, with a slack factor of 0.2, any subsequently assessed performance that is less than 1/(1.2) or 83% of the current best performance triggers the termination policy. An evaluation delay of 6 trials was chosen for generating a robust set of initial performance metrics. The evaluation interval was set as 2, so that evaluation would not consume resources every trial, but would still sample trials at a high enough frequency so as to track performance dips.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
